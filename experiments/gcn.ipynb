{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load DNS and mDNS datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from src.utils import score\n",
    "from src.loader import DNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_path = lambda graph_name: f'../data/{graph_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### mDNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = DNS(root=kg_path('mDNS'), transform=T.Compose([T.NormalizeFeatures(), T.ToUndirected()]), balance_gt=True)\n",
    "# data = dataset[0]\n",
    "# data['domain_node']['test_mask'].unique(return_counts=True)\n",
    "# # datadomain_node.test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### DNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove parallel edges: type\n",
      "similar    50910\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mip_node\u001b[0m={\n",
       "    num_nodes=73593,\n",
       "    x=[73593, 2]\n",
       "  },\n",
       "  \u001b[1mdomain_node\u001b[0m={\n",
       "    num_nodes=373475,\n",
       "    x=[373475, 10],\n",
       "    y=[373475],\n",
       "    train_mask=[373475],\n",
       "    test_mask=[373475],\n",
       "    val_mask=[373475]\n",
       "  },\n",
       "  \u001b[1m(domain_node, apex, domain_node)\u001b[0m={ edge_index=[2, 178944] },\n",
       "  \u001b[1m(domain_node, resolves, ip_node)\u001b[0m={ edge_index=[2, 730438] },\n",
       "  \u001b[1m(domain_node, similar, domain_node)\u001b[0m={ edge_index=[2, 155356] },\n",
       "  \u001b[1m(ip_node, rev_resolves, domain_node)\u001b[0m={ edge_index=[2, 730438] }\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DNS(root=kg_path('DNS'), transform=T.Compose([T.NormalizeFeatures(), T.ToUndirected()]), balance_gt=True)\n",
    "data = dataset[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([False,  True]), tensor([442682,   4386]))\n",
      "(tensor([False,  True]), tensor([444437,   2631]))\n"
     ]
    }
   ],
   "source": [
    "data = dataset.to_homogeneous()\n",
    "print(data.train_mask.unique(return_counts=True))\n",
    "print(data.test_mask.unique(return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv, HeteroConv, Linear\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, num_features, dim=16, num_classes=2, num_layers=2, model_type='gcn'):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.conv1 = SAGEConv(num_features, dim) if model_type == 'sage' else (GCNConv(num_features, dim) if model_type == 'gcn' else GATConv(num_features, dim))\n",
    "        self.gcs = nn.ModuleList()\n",
    "        self.num_layers = num_layers\n",
    "        for i in range(1, num_layers):\n",
    "            conv = SAGEConv(dim, dim) if model_type == 'sage' else (GCNConv(dim, dim) if model_type == 'gcn' else GATConv(dim, dim))\n",
    "            self.gcs.append(conv)\n",
    "        self.lin = Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, data=None, save_embedding=False):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        for i in range(1, self.num_layers):\n",
    "            x = F.relu(self.gcs[i-1](x, edge_index))\n",
    "            x = F.dropout(x, training=self.training)\n",
    "        return self.lin(x)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove parallel edges: type\n",
      "similar    50910\n",
      "dtype: int64\n",
      "Epoch: 000, Loss: 0.7016, Train: 0.5768, Val: 0.5801\n",
      "Epoch: 020, Loss: 0.5693, Train: 0.7004, Val: 0.6986\n",
      "Epoch: 040, Loss: 0.5234, Train: 0.7503, Val: 0.7504\n",
      "Epoch: 060, Loss: 0.4901, Train: 0.7608, Val: 0.7635\n",
      "Epoch: 080, Loss: 0.4654, Train: 0.7695, Val: 0.7675\n",
      "Epoch: 100, Loss: 0.4683, Train: 0.7775, Val: 0.7726\n",
      "Epoch: 120, Loss: 0.4611, Train: 0.7775, Val: 0.7738\n",
      "Epoch: 140, Loss: 0.4548, Train: 0.7816, Val: 0.7818\n",
      "Epoch: 160, Loss: 0.4516, Train: 0.7823, Val: 0.7795\n",
      "Epoch: 180, Loss: 0.4575, Train: 0.7811, Val: 0.7789\n",
      "Epoch: 200, Loss: 0.4527, Train: 0.7839, Val: 0.7823\n",
      "tn, fp, fn, tp 1074 227 390 940\n",
      "acc :0.7655\n",
      "f1 :0.7647\n",
      "auc :0.7661\n",
      "prec :0.8055\n",
      "recall :0.7068\n",
      "fpr :0.1745\n",
      "Remove parallel edges: type\n",
      "similar    50910\n",
      "dtype: int64\n",
      "Epoch: 000, Loss: 0.7042, Train: 0.6835, Val: 0.6860\n",
      "Epoch: 020, Loss: 0.5704, Train: 0.6956, Val: 0.6991\n",
      "Epoch: 040, Loss: 0.5340, Train: 0.7485, Val: 0.7442\n",
      "Epoch: 060, Loss: 0.4970, Train: 0.7592, Val: 0.7533\n",
      "Epoch: 080, Loss: 0.4733, Train: 0.7604, Val: 0.7533\n",
      "Epoch: 100, Loss: 0.4717, Train: 0.7672, Val: 0.7658\n",
      "Epoch: 120, Loss: 0.4636, Train: 0.7745, Val: 0.7715\n",
      "Epoch: 140, Loss: 0.4533, Train: 0.7734, Val: 0.7698\n",
      "Epoch: 160, Loss: 0.4574, Train: 0.7795, Val: 0.7732\n",
      "Epoch: 180, Loss: 0.4562, Train: 0.7800, Val: 0.7744\n",
      "Epoch: 200, Loss: 0.4492, Train: 0.7825, Val: 0.7744\n",
      "tn, fp, fn, tp 1078 245 377 931\n",
      "acc :0.7636\n",
      "f1 :0.7629\n",
      "auc :0.7633\n",
      "prec :0.7917\n",
      "recall :0.7118\n",
      "fpr :0.1852\n",
      "Remove parallel edges: type\n",
      "similar    50910\n",
      "dtype: int64\n",
      "Epoch: 000, Loss: 0.7005, Train: 0.6402, Val: 0.6234\n",
      "Epoch: 020, Loss: 0.5748, Train: 0.7002, Val: 0.6781\n",
      "Epoch: 040, Loss: 0.5219, Train: 0.7538, Val: 0.7316\n",
      "Epoch: 060, Loss: 0.4970, Train: 0.7627, Val: 0.7413\n",
      "Epoch: 080, Loss: 0.4690, Train: 0.7654, Val: 0.7407\n",
      "Epoch: 100, Loss: 0.4709, Train: 0.7670, Val: 0.7396\n",
      "Epoch: 120, Loss: 0.4587, Train: 0.7713, Val: 0.7430\n",
      "Epoch: 140, Loss: 0.4574, Train: 0.7786, Val: 0.7504\n",
      "Epoch: 160, Loss: 0.4502, Train: 0.7786, Val: 0.7521\n",
      "Epoch: 180, Loss: 0.4544, Train: 0.7772, Val: 0.7504\n",
      "Epoch: 200, Loss: 0.4539, Train: 0.7804, Val: 0.7516\n",
      "tn, fp, fn, tp 1080 234 332 985\n",
      "acc :0.7849\n",
      "f1 :0.7846\n",
      "auc :0.7849\n",
      "prec :0.8080\n",
      "recall :0.7479\n",
      "fpr :0.1781\n",
      "Remove parallel edges: type\n",
      "similar    50910\n",
      "dtype: int64\n",
      "Epoch: 000, Loss: 0.6971, Train: 0.6954, Val: 0.6821\n",
      "Epoch: 020, Loss: 0.5757, Train: 0.7000, Val: 0.6940\n",
      "Epoch: 040, Loss: 0.5393, Train: 0.7570, Val: 0.7442\n",
      "Epoch: 060, Loss: 0.4970, Train: 0.7585, Val: 0.7430\n",
      "Epoch: 080, Loss: 0.4876, Train: 0.7633, Val: 0.7453\n",
      "Epoch: 100, Loss: 0.4790, Train: 0.7681, Val: 0.7487\n",
      "Epoch: 120, Loss: 0.4718, Train: 0.7688, Val: 0.7550\n",
      "Epoch: 140, Loss: 0.4682, Train: 0.7718, Val: 0.7550\n",
      "Epoch: 160, Loss: 0.4598, Train: 0.7747, Val: 0.7556\n",
      "Epoch: 180, Loss: 0.4596, Train: 0.7750, Val: 0.7567\n",
      "Epoch: 200, Loss: 0.4610, Train: 0.7752, Val: 0.7607\n",
      "tn, fp, fn, tp 1038 248 344 1001\n",
      "acc :0.7750\n",
      "f1 :0.7749\n",
      "auc :0.7757\n",
      "prec :0.8014\n",
      "recall :0.7442\n",
      "fpr :0.1928\n",
      "Remove parallel edges: type\n",
      "similar    50910\n",
      "dtype: int64\n",
      "Epoch: 000, Loss: 0.6927, Train: 0.5784, Val: 0.5749\n",
      "Epoch: 020, Loss: 0.5731, Train: 0.7016, Val: 0.6991\n",
      "Epoch: 040, Loss: 0.5453, Train: 0.7485, Val: 0.7504\n",
      "Epoch: 060, Loss: 0.5042, Train: 0.7604, Val: 0.7635\n",
      "Epoch: 080, Loss: 0.4874, Train: 0.7604, Val: 0.7584\n",
      "Epoch: 100, Loss: 0.4718, Train: 0.7636, Val: 0.7652\n",
      "Epoch: 120, Loss: 0.4742, Train: 0.7686, Val: 0.7687\n",
      "Epoch: 140, Loss: 0.4655, Train: 0.7674, Val: 0.7675\n",
      "Epoch: 160, Loss: 0.4649, Train: 0.7738, Val: 0.7772\n",
      "Epoch: 180, Loss: 0.4644, Train: 0.7699, Val: 0.7687\n",
      "Epoch: 200, Loss: 0.4602, Train: 0.7745, Val: 0.7789\n",
      "tn, fp, fn, tp 1050 259 326 996\n",
      "acc :0.7777\n",
      "f1 :0.7775\n",
      "auc :0.7778\n",
      "prec :0.7936\n",
      "recall :0.7534\n",
      "fpr :0.1979\n",
      "Remove parallel edges: type\n",
      "similar    50910\n",
      "dtype: int64\n",
      "Epoch: 000, Loss: 0.6967, Train: 0.5752, Val: 0.6023\n",
      "Epoch: 020, Loss: 0.5303, Train: 0.7595, Val: 0.7709\n",
      "Epoch: 040, Loss: 0.4916, Train: 0.7695, Val: 0.7789\n",
      "Epoch: 060, Loss: 0.4770, Train: 0.7786, Val: 0.7789\n",
      "Epoch: 080, Loss: 0.4656, Train: 0.7870, Val: 0.7852\n",
      "Epoch: 100, Loss: 0.4449, Train: 0.7932, Val: 0.7903\n",
      "Epoch: 120, Loss: 0.4430, Train: 0.7925, Val: 0.7903\n",
      "Epoch: 140, Loss: 0.4364, Train: 0.7959, Val: 0.7943\n",
      "Epoch: 160, Loss: 0.4351, Train: 0.7994, Val: 0.7920\n",
      "Epoch: 180, Loss: 0.4380, Train: 0.8019, Val: 0.7960\n",
      "Epoch: 200, Loss: 0.4306, Train: 0.7998, Val: 0.7943\n",
      "tn, fp, fn, tp 1106 180 415 930\n",
      "acc :0.7739\n",
      "f1 :0.7725\n",
      "auc :0.7757\n",
      "prec :0.8378\n",
      "recall :0.6914\n",
      "fpr :0.1400\n",
      "Remove parallel edges: type\n",
      "similar    50910\n",
      "dtype: int64\n",
      "Epoch: 000, Loss: 0.6907, Train: 0.7287, Val: 0.6997\n",
      "Epoch: 020, Loss: 0.5176, Train: 0.7633, Val: 0.7379\n",
      "Epoch: 040, Loss: 0.4949, Train: 0.7688, Val: 0.7436\n",
      "Epoch: 060, Loss: 0.4725, Train: 0.7763, Val: 0.7510\n",
      "Epoch: 080, Loss: 0.4498, Train: 0.7877, Val: 0.7641\n",
      "Epoch: 100, Loss: 0.4491, Train: 0.7893, Val: 0.7584\n",
      "Epoch: 120, Loss: 0.4490, Train: 0.7914, Val: 0.7664\n",
      "Epoch: 140, Loss: 0.4358, Train: 0.7971, Val: 0.7681\n",
      "Epoch: 160, Loss: 0.4336, Train: 0.8005, Val: 0.7744\n",
      "Epoch: 180, Loss: 0.4268, Train: 0.7989, Val: 0.7738\n",
      "Epoch: 200, Loss: 0.4322, Train: 0.8023, Val: 0.7783\n",
      "tn, fp, fn, tp 1106 182 366 977\n",
      "acc :0.7917\n",
      "f1 :0.7910\n",
      "auc :0.7931\n",
      "prec :0.8430\n",
      "recall :0.7275\n",
      "fpr :0.1413\n",
      "Remove parallel edges: type\n",
      "similar    50910\n",
      "dtype: int64\n",
      "Epoch: 000, Loss: 0.6961, Train: 0.7262, Val: 0.7174\n",
      "Epoch: 020, Loss: 0.5094, Train: 0.7501, Val: 0.7476\n",
      "Epoch: 040, Loss: 0.4980, Train: 0.7633, Val: 0.7584\n",
      "Epoch: 060, Loss: 0.4745, Train: 0.7738, Val: 0.7698\n",
      "Epoch: 080, Loss: 0.4648, Train: 0.7836, Val: 0.7721\n",
      "Epoch: 100, Loss: 0.4523, Train: 0.7925, Val: 0.7726\n",
      "Epoch: 120, Loss: 0.4502, Train: 0.7916, Val: 0.7692\n",
      "Epoch: 140, Loss: 0.4333, Train: 0.7939, Val: 0.7709\n",
      "Epoch: 160, Loss: 0.4424, Train: 0.7989, Val: 0.7749\n",
      "Epoch: 180, Loss: 0.4346, Train: 0.7959, Val: 0.7732\n",
      "Epoch: 200, Loss: 0.4292, Train: 0.7964, Val: 0.7744\n",
      "tn, fp, fn, tp 1098 231 287 1015\n",
      "acc :0.8031\n",
      "f1 :0.8030\n",
      "auc :0.8029\n",
      "prec :0.8146\n",
      "recall :0.7796\n",
      "fpr :0.1738\n",
      "Remove parallel edges: type\n",
      "similar    50910\n",
      "dtype: int64\n",
      "Epoch: 000, Loss: 0.6957, Train: 0.7348, Val: 0.7419\n",
      "Epoch: 020, Loss: 0.5149, Train: 0.7617, Val: 0.7670\n",
      "Epoch: 040, Loss: 0.4883, Train: 0.7747, Val: 0.7721\n",
      "Epoch: 060, Loss: 0.4695, Train: 0.7818, Val: 0.7766\n",
      "Epoch: 080, Loss: 0.4533, Train: 0.7855, Val: 0.7721\n",
      "Epoch: 100, Loss: 0.4500, Train: 0.7969, Val: 0.7755\n",
      "Epoch: 120, Loss: 0.4404, Train: 0.7975, Val: 0.7766\n",
      "Epoch: 140, Loss: 0.4412, Train: 0.7989, Val: 0.7846\n",
      "Epoch: 160, Loss: 0.4379, Train: 0.7996, Val: 0.7806\n",
      "Epoch: 180, Loss: 0.4414, Train: 0.7998, Val: 0.7812\n",
      "Epoch: 200, Loss: 0.4377, Train: 0.8010, Val: 0.7806\n",
      "tn, fp, fn, tp 1088 201 356 986\n",
      "acc :0.7883\n",
      "f1 :0.7878\n",
      "auc :0.7894\n",
      "prec :0.8307\n",
      "recall :0.7347\n",
      "fpr :0.1559\n",
      "Remove parallel edges: type\n",
      "similar    50910\n",
      "dtype: int64\n",
      "Epoch: 000, Loss: 0.7068, Train: 0.5862, Val: 0.5778\n",
      "Epoch: 020, Loss: 0.5253, Train: 0.7506, Val: 0.7487\n",
      "Epoch: 040, Loss: 0.5036, Train: 0.7622, Val: 0.7618\n",
      "Epoch: 060, Loss: 0.4853, Train: 0.7734, Val: 0.7715\n",
      "Epoch: 080, Loss: 0.4741, Train: 0.7802, Val: 0.7812\n",
      "Epoch: 100, Loss: 0.4657, Train: 0.7855, Val: 0.7761\n",
      "Epoch: 120, Loss: 0.4603, Train: 0.7864, Val: 0.7726\n",
      "Epoch: 140, Loss: 0.4494, Train: 0.7930, Val: 0.7806\n",
      "Epoch: 160, Loss: 0.4444, Train: 0.7980, Val: 0.7812\n",
      "Epoch: 180, Loss: 0.4455, Train: 0.7964, Val: 0.7812\n",
      "Epoch: 200, Loss: 0.4412, Train: 0.7978, Val: 0.7835\n",
      "tn, fp, fn, tp 1117 205 338 971\n",
      "acc :0.7936\n",
      "f1 :0.7930\n",
      "auc :0.7934\n",
      "prec :0.8257\n",
      "recall :0.7418\n",
      "fpr :0.1551\n"
     ]
    }
   ],
   "source": [
    "cuda_device = 3\n",
    "torch.manual_seed(42)\n",
    "from src.utils import score\n",
    "\n",
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    mask = data.train_mask\n",
    "    loss = F.cross_entropy(out[mask], data.y[mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    pred = model(data.x, data.edge_index).argmax(dim=-1)\n",
    "\n",
    "    accs = []\n",
    "    for split in ['train_mask', 'val_mask']:\n",
    "        mask = data[split]\n",
    "        acc = (pred[mask] == data.y[mask]).sum() / mask.sum()\n",
    "        accs.append(float(acc))\n",
    "    return accs\n",
    "\n",
    "def experiment(model,start,end,test_list, model_type):\n",
    "    kg_path = lambda graph_name: f'../data/{graph_name}'\n",
    "\n",
    "    dataset = DNS(root=kg_path('DNS'), transform=T.Compose([T.NormalizeFeatures(), T.ToUndirected()]), balance_gt=True)\n",
    "    data = dataset.to_homogeneous() # training data\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        torch.cuda.set_device(cuda_device)\n",
    "\n",
    "        data, model = data.to(device), model.to(device)\n",
    "\n",
    "    with torch.no_grad():  # Initialize lazy modules.\n",
    "        out = model(data.x, data.edge_index)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "\n",
    "    for epoch in range(0, 201):\n",
    "        loss = train(model, data, optimizer)\n",
    "        train_acc, val_acc = test(model,data)\n",
    "        if epoch % 20 == 0:\n",
    "            print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, '\n",
    "                f'Val: {val_acc:.4f}')\n",
    "        \n",
    "    model.eval()\n",
    "    test_data = data\n",
    "    test_data = test_data.to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(test_data.x, test_data.edge_index).argmax(dim=-1)\n",
    "    mask = test_data['test_mask']\n",
    "    scores = score(pred[mask],test_data.y[mask])\n",
    "    with open(\"resultsdns_copy.csv\", \"a\") as logger:\n",
    "        logger.write(model_type + ',')\n",
    "        logger.write(\",\".join(str(x) for x in scores.values()))\n",
    "        logger.write('\\n')\n",
    "\n",
    "\n",
    "    for metric, val in scores.items():\n",
    "        print(metric, ':{:.4f}'.format(val))\n",
    "    \n",
    "for model_type in ['gcn','sage']:\n",
    "    for i in range(5):\n",
    "        # model_type='gcn'  \n",
    "        data.x.size(1) \n",
    "        model = GNN(data.x.size(1), dim=64, num_classes=2,\n",
    "                  num_layers=2, model_type=model_type)\n",
    "        experiment(model,i,i+6,[i+7,i+8], model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Loss: 0.5827, Train: 0.7020, Val: 0.7003, Test: 0.7001\n",
      "Epoch: 040, Loss: 0.5217, Train: 0.7540, Val: 0.7544, Test: 0.7480\n",
      "Epoch: 060, Loss: 0.4907, Train: 0.7620, Val: 0.7658, Test: 0.7613\n",
      "Epoch: 080, Loss: 0.4746, Train: 0.7693, Val: 0.7698, Test: 0.7662\n",
      "Epoch: 100, Loss: 0.4783, Train: 0.7727, Val: 0.7744, Test: 0.7723\n",
      "Epoch: 120, Loss: 0.4774, Train: 0.7729, Val: 0.7772, Test: 0.7685\n",
      "Epoch: 140, Loss: 0.4732, Train: 0.7763, Val: 0.7801, Test: 0.7739\n",
      "Epoch: 160, Loss: 0.4745, Train: 0.7718, Val: 0.7766, Test: 0.7681\n",
      "Epoch: 180, Loss: 0.4666, Train: 0.7756, Val: 0.7795, Test: 0.7704\n",
      "Epoch: 200, Loss: 0.4652, Train: 0.7763, Val: 0.7783, Test: 0.7674\n",
      "tn, fp, fn, tp 1079 222 390 940\n",
      "acc :0.77\n",
      "f1 :0.77\n",
      "auc :0.77\n",
      "prec :0.81\n",
      "recall :0.71\n",
      "fpr :0.17\n"
     ]
    }
   ],
   "source": [
    "# cuda_device = 3\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "# model = GNN(data.x.size(1), dim=64, num_classes=2,\n",
    "#                   num_layers=2, model_type='gcn')\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     torch.cuda.set_device(cuda_device)\n",
    "\n",
    "#     data, model = data.to(device), model.to(device)\n",
    "\n",
    "# with torch.no_grad():  # Initialize lazy modules.\n",
    "#     out = model(data.x, data.edge_index)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "\n",
    "# def train(model, optimizer, data):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     out = model(data.x, data.edge_index)\n",
    "#     mask = data.train_mask\n",
    "#     loss = F.cross_entropy(out[mask], data.y[mask])\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     return float(loss)\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def test():\n",
    "#     model.eval()\n",
    "#     pred = model(data.x, data.edge_index).argmax(dim=-1)\n",
    "\n",
    "#     accs = []\n",
    "#     for split in ['train_mask', 'val_mask', 'test_mask']:\n",
    "#         mask = data[split]\n",
    "#         acc = (pred[mask] == data.y[mask]).sum() / mask.sum()\n",
    "#         accs.append(float(acc))\n",
    "#     return accs\n",
    "\n",
    "\n",
    "# for epoch in range(1, 201):\n",
    "#     loss = train()\n",
    "#     train_acc, val_acc, test_acc = test()\n",
    "#     if epoch % 20 == 0:\n",
    "#         print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, '\n",
    "#               f'Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
    "    \n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     pred = model(data.x, data.edge_index).argmax(dim=-1)\n",
    "# mask = data['test_mask']\n",
    "# scores = score(pred[mask],data.y[mask])\n",
    "# with open(\"resultsdns_copy.csv\", \"a\") as logger:\n",
    "#             logger.write('gcn,')\n",
    "#             logger.write(\",\".join(str(x) for x in scores.values()))\n",
    "#             logger.write('\\n')\n",
    "# for metric, score in scores.items():\n",
    "#     print(metric, ':{:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
