{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load DNS and mDNS datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('.'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import torch\n",
    "import src.temporal_loader_v2 as tl\n",
    "from src.utils import to_homogeneous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbb664c1790>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda_device = 4\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    torch.cuda.set_device(cuda_device)\n",
    "    \n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total labeled 897635\n",
      "Labeled node count for 0, 6: 31778\n",
      "After balancing labeled count: 31282\n",
      "Labeled node count for 0, 7: 2610\n"
     ]
    }
   ],
   "source": [
    "kg_path = lambda graph_name: f'./data/{graph_name}'\n",
    "dataset = tl.DNS(root=kg_path('DNS_2m'), start=0, end=6, test_list=[7], balance_gt=True, domain_file='domains2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### DNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1173558, 12])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = to_homogeneous(dataset.train_data) # training data\n",
    "# test_data = to_homogeneous(dataset.test_data[0])\n",
    "data.x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv, HeteroConv, Linear\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, num_features, dim=16, num_classes=2, num_layers=2, model_type='gcn'):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.conv1 = SAGEConv(num_features, dim) if model_type == 'sage' else (GCNConv(num_features, dim) if model_type == 'gcn' else GATConv(num_features, dim))\n",
    "        self.gcs = nn.ModuleList()\n",
    "        self.num_layers = num_layers\n",
    "        for i in range(1, num_layers):\n",
    "            conv = SAGEConv(dim, dim) if model_type == 'sage' else (GCNConv(dim, dim) if model_type == 'gcn' else GATConv(dim, dim))\n",
    "            self.gcs.append(conv)\n",
    "        self.lin = Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, data=None, save_embedding=False):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        for i in range(1, self.num_layers):\n",
    "            x = F.relu(self.gcs[i-1](x, edge_index))\n",
    "            x = F.dropout(x, training=self.training)\n",
    "        return self.lin(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total labeled 897635\n",
      "Labeled node count for 0, 6: 31778\n",
      "Labeled node count for 0, 7: 2610\n",
      "Labeled node count for 0, 8: 2083\n",
      "Epoch: 000, Loss: 0.6785, Train: 0.7045, Val: 0.7054\n",
      "Epoch: 020, Loss: 0.4970, Train: 0.7824, Val: 0.7784\n",
      "Epoch: 040, Loss: 0.4525, Train: 0.8070, Val: 0.8050\n",
      "Epoch: 060, Loss: 0.4291, Train: 0.8095, Val: 0.8077\n",
      "Epoch: 080, Loss: 0.4184, Train: 0.8113, Val: 0.8094\n",
      "Epoch: 100, Loss: 0.4154, Train: 0.8105, Val: 0.8094\n",
      "Epoch: 120, Loss: 0.4131, Train: 0.8125, Val: 0.8105\n",
      "Epoch: 140, Loss: 0.4110, Train: 0.8123, Val: 0.8101\n",
      "Epoch: 160, Loss: 0.4111, Train: 0.8128, Val: 0.8105\n",
      "Epoch: 180, Loss: 0.4101, Train: 0.8126, Val: 0.8109\n",
      "Epoch: 200, Loss: 0.4098, Train: 0.8126, Val: 0.8107\n",
      "tn, fp, fn, tp 1315 290 171 834\n",
      "acc :0.8234\n",
      "f1 :0.8249\n",
      "auc :0.8246\n",
      "prec :0.7420\n",
      "recall :0.8299\n",
      "fpr :0.1807\n",
      "mi_f1 :0.8234\n",
      "ma_f1 :0.8172\n",
      "tn, fp, fn, tp 1065 244 121 653\n",
      "acc :0.8248\n",
      "f1 :0.8269\n",
      "auc :0.8286\n",
      "prec :0.7280\n",
      "recall :0.8437\n",
      "fpr :0.1864\n",
      "mi_f1 :0.8248\n",
      "ma_f1 :0.8176\n",
      "Total labeled 897635\n",
      "Labeled node count for 1, 7: 32913\n",
      "Labeled node count for 1, 8: 2164\n",
      "Labeled node count for 1, 9: 2756\n",
      "Epoch: 000, Loss: 0.6918, Train: 0.6694, Val: 0.6651\n",
      "Epoch: 020, Loss: 0.5076, Train: 0.7700, Val: 0.7644\n",
      "Epoch: 040, Loss: 0.4616, Train: 0.7965, Val: 0.7969\n",
      "Epoch: 060, Loss: 0.4343, Train: 0.8092, Val: 0.8099\n",
      "Epoch: 080, Loss: 0.4235, Train: 0.8100, Val: 0.8078\n",
      "Epoch: 100, Loss: 0.4172, Train: 0.8108, Val: 0.8101\n",
      "Epoch: 120, Loss: 0.4172, Train: 0.8122, Val: 0.8104\n",
      "Epoch: 140, Loss: 0.4128, Train: 0.8125, Val: 0.8099\n",
      "Epoch: 160, Loss: 0.4140, Train: 0.8123, Val: 0.8107\n",
      "Epoch: 180, Loss: 0.4149, Train: 0.8117, Val: 0.8096\n",
      "Epoch: 200, Loss: 0.4101, Train: 0.8126, Val: 0.8115\n",
      "tn, fp, fn, tp 1098 241 137 688\n",
      "acc :0.8253\n",
      "f1 :0.8270\n",
      "auc :0.8270\n",
      "prec :0.7406\n",
      "recall :0.8339\n",
      "fpr :0.1800\n",
      "mi_f1 :0.8253\n",
      "ma_f1 :0.8188\n",
      "tn, fp, fn, tp 1180 237 248 1091\n",
      "acc :0.8240\n",
      "f1 :0.8240\n",
      "auc :0.8238\n",
      "prec :0.8215\n",
      "recall :0.8148\n",
      "fpr :0.1673\n",
      "mi_f1 :0.8240\n",
      "ma_f1 :0.8238\n",
      "Total labeled 897635\n",
      "Labeled node count for 2, 8: 33022\n",
      "Labeled node count for 2, 9: 2878\n",
      "Labeled node count for 2, 10: 1831\n",
      "Epoch: 000, Loss: 0.6917, Train: 0.7057, Val: 0.7067\n",
      "Epoch: 020, Loss: 0.5097, Train: 0.7880, Val: 0.7848\n",
      "Epoch: 040, Loss: 0.4552, Train: 0.8013, Val: 0.8038\n",
      "Epoch: 060, Loss: 0.4257, Train: 0.8111, Val: 0.8089\n",
      "Epoch: 080, Loss: 0.4210, Train: 0.8124, Val: 0.8122\n",
      "Epoch: 100, Loss: 0.4151, Train: 0.8132, Val: 0.8141\n",
      "Epoch: 120, Loss: 0.4099, Train: 0.8136, Val: 0.8136\n",
      "Epoch: 140, Loss: 0.4097, Train: 0.8140, Val: 0.8137\n",
      "Epoch: 160, Loss: 0.4087, Train: 0.8135, Val: 0.8136\n",
      "Epoch: 180, Loss: 0.4072, Train: 0.8135, Val: 0.8147\n",
      "Epoch: 200, Loss: 0.4066, Train: 0.8137, Val: 0.8141\n",
      "tn, fp, fn, tp 1242 250 249 1137\n",
      "acc :0.8266\n",
      "f1 :0.8266\n",
      "auc :0.8264\n",
      "prec :0.8198\n",
      "recall :0.8203\n",
      "fpr :0.1676\n",
      "mi_f1 :0.8266\n",
      "ma_f1 :0.8264\n",
      "tn, fp, fn, tp 812 163 127 729\n",
      "acc :0.8416\n",
      "f1 :0.8418\n",
      "auc :0.8422\n",
      "prec :0.8173\n",
      "recall :0.8516\n",
      "fpr :0.1672\n",
      "mi_f1 :0.8416\n",
      "ma_f1 :0.8413\n",
      "Total labeled 897635\n",
      "Labeled node count for 3, 9: 33643\n",
      "Labeled node count for 3, 10: 1906\n",
      "Labeled node count for 3, 11: 1460\n",
      "Epoch: 000, Loss: 0.6897, Train: 0.7111, Val: 0.7045\n",
      "Epoch: 020, Loss: 0.5044, Train: 0.7883, Val: 0.7839\n",
      "Epoch: 040, Loss: 0.4497, Train: 0.8056, Val: 0.8007\n",
      "Epoch: 060, Loss: 0.4219, Train: 0.8109, Val: 0.8086\n",
      "Epoch: 080, Loss: 0.4139, Train: 0.8128, Val: 0.8065\n",
      "Epoch: 100, Loss: 0.4095, Train: 0.8134, Val: 0.8083\n",
      "Epoch: 120, Loss: 0.4098, Train: 0.8141, Val: 0.8077\n",
      "Epoch: 140, Loss: 0.4065, Train: 0.8144, Val: 0.8080\n",
      "Epoch: 160, Loss: 0.4073, Train: 0.8140, Val: 0.8092\n",
      "Epoch: 180, Loss: 0.4070, Train: 0.8148, Val: 0.8089\n",
      "Epoch: 200, Loss: 0.4059, Train: 0.8148, Val: 0.8093\n",
      "tn, fp, fn, tp 830 173 132 771\n",
      "acc :0.8400\n",
      "f1 :0.8401\n",
      "auc :0.8407\n",
      "prec :0.8167\n",
      "recall :0.8538\n",
      "fpr :0.1725\n",
      "mi_f1 :0.8400\n",
      "ma_f1 :0.8398\n",
      "tn, fp, fn, tp 757 168 97 438\n",
      "acc :0.8185\n",
      "f1 :0.8205\n",
      "auc :0.8185\n",
      "prec :0.7228\n",
      "recall :0.8187\n",
      "fpr :0.1816\n",
      "mi_f1 :0.8185\n",
      "ma_f1 :0.8094\n",
      "Total labeled 897635\n",
      "Labeled node count for 4, 10: 32652\n",
      "Labeled node count for 4, 11: 1530\n",
      "Labeled node count for 4, 12: 2044\n",
      "Epoch: 000, Loss: 0.6902, Train: 0.5695, Val: 0.5893\n",
      "Epoch: 020, Loss: 0.5014, Train: 0.7858, Val: 0.7836\n",
      "Epoch: 040, Loss: 0.4371, Train: 0.8097, Val: 0.8106\n",
      "Epoch: 060, Loss: 0.4234, Train: 0.8122, Val: 0.8103\n",
      "Epoch: 080, Loss: 0.4131, Train: 0.8136, Val: 0.8135\n",
      "Epoch: 100, Loss: 0.4093, Train: 0.8146, Val: 0.8139\n",
      "Epoch: 120, Loss: 0.4099, Train: 0.8143, Val: 0.8142\n",
      "Epoch: 140, Loss: 0.4059, Train: 0.8152, Val: 0.8155\n",
      "Epoch: 160, Loss: 0.4048, Train: 0.8153, Val: 0.8155\n",
      "Epoch: 180, Loss: 0.4042, Train: 0.8160, Val: 0.8159\n",
      "Epoch: 200, Loss: 0.4064, Train: 0.8156, Val: 0.8149\n",
      "tn, fp, fn, tp 778 181 96 475\n",
      "acc :0.8190\n",
      "f1 :0.8210\n",
      "auc :0.8216\n",
      "prec :0.7241\n",
      "recall :0.8319\n",
      "fpr :0.1887\n",
      "mi_f1 :0.8190\n",
      "ma_f1 :0.8116\n",
      "tn, fp, fn, tp 947 209 172 716\n",
      "acc :0.8136\n",
      "f1 :0.8140\n",
      "auc :0.8128\n",
      "prec :0.7741\n",
      "recall :0.8063\n",
      "fpr :0.1808\n",
      "mi_f1 :0.8136\n",
      "ma_f1 :0.8112\n",
      "Total labeled 897635\n",
      "Labeled node count for 0, 6: 31778\n",
      "Labeled node count for 0, 7: 2610\n",
      "Labeled node count for 0, 8: 2083\n",
      "Epoch: 000, Loss: 0.7091, Train: 0.5501, Val: 0.5481\n",
      "Epoch: 020, Loss: 0.4929, Train: 0.7648, Val: 0.7619\n",
      "Epoch: 040, Loss: 0.4438, Train: 0.8071, Val: 0.8050\n",
      "Epoch: 060, Loss: 0.4205, Train: 0.8150, Val: 0.8156\n",
      "Epoch: 080, Loss: 0.4092, Train: 0.8181, Val: 0.8170\n",
      "Epoch: 100, Loss: 0.4039, Train: 0.8185, Val: 0.8172\n",
      "Epoch: 120, Loss: 0.4016, Train: 0.8195, Val: 0.8187\n",
      "Epoch: 140, Loss: 0.3980, Train: 0.8189, Val: 0.8183\n",
      "Epoch: 160, Loss: 0.3987, Train: 0.8206, Val: 0.8190\n",
      "Epoch: 180, Loss: 0.3961, Train: 0.8197, Val: 0.8197\n",
      "Epoch: 200, Loss: 0.3951, Train: 0.8200, Val: 0.8197\n",
      "tn, fp, fn, tp 1318 287 129 876\n",
      "acc :0.8406\n",
      "f1 :0.8423\n",
      "auc :0.8464\n",
      "prec :0.7532\n",
      "recall :0.8716\n",
      "fpr :0.1788\n",
      "mi_f1 :0.8406\n",
      "ma_f1 :0.8359\n",
      "tn, fp, fn, tp 1073 236 118 656\n",
      "acc :0.8301\n",
      "f1 :0.8321\n",
      "auc :0.8336\n",
      "prec :0.7354\n",
      "recall :0.8475\n",
      "fpr :0.1803\n",
      "mi_f1 :0.8301\n",
      "ma_f1 :0.8230\n",
      "Total labeled 897635\n",
      "Labeled node count for 1, 7: 32913\n",
      "Labeled node count for 1, 8: 2164\n",
      "Labeled node count for 1, 9: 2756\n",
      "Epoch: 000, Loss: 0.7006, Train: 0.5427, Val: 0.5334\n",
      "Epoch: 020, Loss: 0.4866, Train: 0.7682, Val: 0.7786\n",
      "Epoch: 040, Loss: 0.4386, Train: 0.8101, Val: 0.8177\n",
      "Epoch: 060, Loss: 0.4169, Train: 0.8145, Val: 0.8224\n",
      "Epoch: 080, Loss: 0.4063, Train: 0.8170, Val: 0.8233\n",
      "Epoch: 100, Loss: 0.4022, Train: 0.8184, Val: 0.8247\n",
      "Epoch: 120, Loss: 0.3980, Train: 0.8187, Val: 0.8236\n",
      "Epoch: 140, Loss: 0.3972, Train: 0.8186, Val: 0.8236\n",
      "Epoch: 160, Loss: 0.3962, Train: 0.8192, Val: 0.8253\n",
      "Epoch: 180, Loss: 0.3937, Train: 0.8194, Val: 0.8257\n",
      "Epoch: 200, Loss: 0.3919, Train: 0.8193, Val: 0.8254\n",
      "tn, fp, fn, tp 1100 239 122 703\n",
      "acc :0.8332\n",
      "f1 :0.8349\n",
      "auc :0.8368\n",
      "prec :0.7463\n",
      "recall :0.8521\n",
      "fpr :0.1785\n",
      "mi_f1 :0.8332\n",
      "ma_f1 :0.8274\n",
      "tn, fp, fn, tp 1183 234 178 1161\n",
      "acc :0.8505\n",
      "f1 :0.8505\n",
      "auc :0.8510\n",
      "prec :0.8323\n",
      "recall :0.8671\n",
      "fpr :0.1651\n",
      "mi_f1 :0.8505\n",
      "ma_f1 :0.8505\n",
      "Total labeled 897635\n",
      "Labeled node count for 2, 8: 33022\n",
      "Labeled node count for 2, 9: 2878\n",
      "Labeled node count for 2, 10: 1831\n",
      "Epoch: 000, Loss: 0.6933, Train: 0.6427, Val: 0.6452\n",
      "Epoch: 020, Loss: 0.4853, Train: 0.7688, Val: 0.7680\n",
      "Epoch: 040, Loss: 0.4447, Train: 0.8096, Val: 0.8098\n",
      "Epoch: 060, Loss: 0.4219, Train: 0.8156, Val: 0.8148\n",
      "Epoch: 080, Loss: 0.4069, Train: 0.8178, Val: 0.8180\n",
      "Epoch: 100, Loss: 0.4029, Train: 0.8184, Val: 0.8162\n",
      "Epoch: 120, Loss: 0.3982, Train: 0.8187, Val: 0.8166\n",
      "Epoch: 140, Loss: 0.3951, Train: 0.8195, Val: 0.8172\n",
      "Epoch: 160, Loss: 0.3940, Train: 0.8201, Val: 0.8171\n",
      "Epoch: 180, Loss: 0.3941, Train: 0.8196, Val: 0.8171\n",
      "Epoch: 200, Loss: 0.3953, Train: 0.8203, Val: 0.8183\n",
      "tn, fp, fn, tp 1247 245 181 1205\n",
      "acc :0.8520\n",
      "f1 :0.8520\n",
      "auc :0.8526\n",
      "prec :0.8310\n",
      "recall :0.8694\n",
      "fpr :0.1642\n",
      "mi_f1 :0.8520\n",
      "ma_f1 :0.8519\n",
      "tn, fp, fn, tp 817 158 107 749\n",
      "acc :0.8553\n",
      "f1 :0.8554\n",
      "auc :0.8565\n",
      "prec :0.8258\n",
      "recall :0.8750\n",
      "fpr :0.1621\n",
      "mi_f1 :0.8553\n",
      "ma_f1 :0.8551\n",
      "Total labeled 897635\n",
      "Labeled node count for 3, 9: 33643\n",
      "Labeled node count for 3, 10: 1906\n",
      "Labeled node count for 3, 11: 1460\n",
      "Epoch: 000, Loss: 0.7010, Train: 0.6122, Val: 0.6125\n",
      "Epoch: 020, Loss: 0.4777, Train: 0.8022, Val: 0.7947\n",
      "Epoch: 040, Loss: 0.4356, Train: 0.8116, Val: 0.8071\n",
      "Epoch: 060, Loss: 0.4120, Train: 0.8185, Val: 0.8112\n",
      "Epoch: 080, Loss: 0.4010, Train: 0.8202, Val: 0.8161\n",
      "Epoch: 100, Loss: 0.3969, Train: 0.8210, Val: 0.8172\n",
      "Epoch: 120, Loss: 0.3936, Train: 0.8223, Val: 0.8202\n",
      "Epoch: 140, Loss: 0.3903, Train: 0.8216, Val: 0.8206\n",
      "Epoch: 160, Loss: 0.3897, Train: 0.8224, Val: 0.8173\n",
      "Epoch: 180, Loss: 0.3921, Train: 0.8218, Val: 0.8184\n",
      "Epoch: 200, Loss: 0.3922, Train: 0.8214, Val: 0.8203\n",
      "tn, fp, fn, tp 844 159 117 786\n",
      "acc :0.8552\n",
      "f1 :0.8553\n",
      "auc :0.8560\n",
      "prec :0.8317\n",
      "recall :0.8704\n",
      "fpr :0.1585\n",
      "mi_f1 :0.8552\n",
      "ma_f1 :0.8551\n",
      "tn, fp, fn, tp 758 167 75 460\n",
      "acc :0.8342\n",
      "f1 :0.8365\n",
      "auc :0.8396\n",
      "prec :0.7337\n",
      "recall :0.8598\n",
      "fpr :0.1805\n",
      "mi_f1 :0.8342\n",
      "ma_f1 :0.8270\n",
      "Total labeled 897635\n",
      "Labeled node count for 4, 10: 32652\n",
      "Labeled node count for 4, 11: 1530\n",
      "Labeled node count for 4, 12: 2044\n",
      "Epoch: 000, Loss: 0.7008, Train: 0.5551, Val: 0.5591\n",
      "Epoch: 020, Loss: 0.4739, Train: 0.8107, Val: 0.8080\n",
      "Epoch: 040, Loss: 0.4382, Train: 0.8128, Val: 0.8129\n",
      "Epoch: 060, Loss: 0.4131, Train: 0.8184, Val: 0.8150\n",
      "Epoch: 080, Loss: 0.4035, Train: 0.8216, Val: 0.8165\n",
      "Epoch: 100, Loss: 0.3957, Train: 0.8225, Val: 0.8176\n",
      "Epoch: 120, Loss: 0.3935, Train: 0.8230, Val: 0.8173\n",
      "Epoch: 140, Loss: 0.3882, Train: 0.8236, Val: 0.8173\n",
      "Epoch: 160, Loss: 0.3887, Train: 0.8240, Val: 0.8167\n",
      "Epoch: 180, Loss: 0.3900, Train: 0.8247, Val: 0.8182\n",
      "Epoch: 200, Loss: 0.3876, Train: 0.8247, Val: 0.8184\n",
      "tn, fp, fn, tp 781 178 72 499\n",
      "acc :0.8366\n",
      "f1 :0.8388\n",
      "auc :0.8441\n",
      "prec :0.7371\n",
      "recall :0.8739\n",
      "fpr :0.1856\n",
      "mi_f1 :0.8366\n",
      "ma_f1 :0.8309\n",
      "tn, fp, fn, tp 950 206 139 749\n",
      "acc :0.8312\n",
      "f1 :0.8318\n",
      "auc :0.8326\n",
      "prec :0.7843\n",
      "recall :0.8435\n",
      "fpr :0.1782\n",
      "mi_f1 :0.8312\n",
      "ma_f1 :0.8296\n",
      "Total labeled 897635\n",
      "Labeled node count for 0, 6: 31778\n",
      "Labeled node count for 0, 7: 2610\n",
      "Labeled node count for 0, 8: 2083\n",
      "Epoch: 000, Loss: 0.6997, Train: 0.5012, Val: 0.4899\n",
      "Epoch: 020, Loss: 0.5078, Train: 0.7804, Val: 0.7772\n",
      "Epoch: 040, Loss: 0.4709, Train: 0.7942, Val: 0.7912\n",
      "Epoch: 060, Loss: 0.4475, Train: 0.7972, Val: 0.7931\n",
      "Epoch: 080, Loss: 0.4346, Train: 0.8027, Val: 0.7980\n",
      "Epoch: 100, Loss: 0.4287, Train: 0.8033, Val: 0.7995\n",
      "Epoch: 120, Loss: 0.4289, Train: 0.8034, Val: 0.8000\n",
      "Epoch: 140, Loss: 0.4262, Train: 0.8035, Val: 0.7994\n",
      "Epoch: 160, Loss: 0.4267, Train: 0.8040, Val: 0.7995\n",
      "Epoch: 180, Loss: 0.4263, Train: 0.8041, Val: 0.7989\n",
      "Epoch: 200, Loss: 0.4235, Train: 0.8014, Val: 0.7986\n",
      "tn, fp, fn, tp 1270 335 161 844\n",
      "acc :0.8100\n",
      "f1 :0.8121\n",
      "auc :0.8155\n",
      "prec :0.7159\n",
      "recall :0.8398\n",
      "fpr :0.2087\n",
      "mi_f1 :0.8100\n",
      "ma_f1 :0.8048\n",
      "tn, fp, fn, tp 1041 268 108 666\n",
      "acc :0.8195\n",
      "f1 :0.8221\n",
      "auc :0.8279\n",
      "prec :0.7131\n",
      "recall :0.8605\n",
      "fpr :0.2047\n",
      "mi_f1 :0.8195\n",
      "ma_f1 :0.8134\n",
      "Total labeled 897635\n",
      "Labeled node count for 1, 7: 32913\n",
      "Labeled node count for 1, 8: 2164\n",
      "Labeled node count for 1, 9: 2756\n",
      "Epoch: 000, Loss: 0.6928, Train: 0.6860, Val: 0.6904\n",
      "Epoch: 020, Loss: 0.5093, Train: 0.7358, Val: 0.7334\n",
      "Epoch: 040, Loss: 0.4739, Train: 0.7881, Val: 0.7850\n",
      "Epoch: 060, Loss: 0.4514, Train: 0.7940, Val: 0.7932\n",
      "Epoch: 080, Loss: 0.4346, Train: 0.8020, Val: 0.8022\n",
      "Epoch: 100, Loss: 0.4305, Train: 0.8027, Val: 0.8029\n",
      "Epoch: 120, Loss: 0.4302, Train: 0.8030, Val: 0.8029\n",
      "Epoch: 140, Loss: 0.4283, Train: 0.8030, Val: 0.8033\n",
      "Epoch: 160, Loss: 0.4269, Train: 0.8029, Val: 0.8033\n",
      "Epoch: 180, Loss: 0.4261, Train: 0.8030, Val: 0.8037\n",
      "Epoch: 200, Loss: 0.4258, Train: 0.8033, Val: 0.8045\n",
      "tn, fp, fn, tp 1078 261 128 697\n",
      "acc :0.8202\n",
      "f1 :0.8222\n",
      "auc :0.8250\n",
      "prec :0.7276\n",
      "recall :0.8448\n",
      "fpr :0.1949\n",
      "mi_f1 :0.8202\n",
      "ma_f1 :0.8145\n",
      "tn, fp, fn, tp 1149 268 318 1021\n",
      "acc :0.7874\n",
      "f1 :0.7872\n",
      "auc :0.7867\n",
      "prec :0.7921\n",
      "recall :0.7625\n",
      "fpr :0.1891\n",
      "mi_f1 :0.7874\n",
      "ma_f1 :0.7869\n",
      "Total labeled 897635\n",
      "Labeled node count for 2, 8: 33022\n",
      "Labeled node count for 2, 9: 2878\n",
      "Labeled node count for 2, 10: 1831\n",
      "Epoch: 000, Loss: 0.6873, Train: 0.7135, Val: 0.7100\n",
      "Epoch: 020, Loss: 0.5003, Train: 0.7743, Val: 0.7741\n",
      "Epoch: 040, Loss: 0.4679, Train: 0.7931, Val: 0.7894\n",
      "Epoch: 060, Loss: 0.4426, Train: 0.8010, Val: 0.7971\n",
      "Epoch: 080, Loss: 0.4337, Train: 0.8037, Val: 0.7985\n",
      "Epoch: 100, Loss: 0.4299, Train: 0.8044, Val: 0.7992\n",
      "Epoch: 120, Loss: 0.4286, Train: 0.8053, Val: 0.7995\n",
      "Epoch: 140, Loss: 0.4262, Train: 0.8053, Val: 0.8000\n",
      "Epoch: 160, Loss: 0.4241, Train: 0.8058, Val: 0.8009\n",
      "Epoch: 180, Loss: 0.4232, Train: 0.8030, Val: 0.7980\n",
      "Epoch: 200, Loss: 0.4235, Train: 0.8034, Val: 0.7977\n",
      "tn, fp, fn, tp 1201 291 237 1149\n",
      "acc :0.8165\n",
      "f1 :0.8166\n",
      "auc :0.8170\n",
      "prec :0.7979\n",
      "recall :0.8290\n",
      "fpr :0.1950\n",
      "mi_f1 :0.8165\n",
      "ma_f1 :0.8165\n",
      "tn, fp, fn, tp 792 183 120 736\n",
      "acc :0.8345\n",
      "f1 :0.8347\n",
      "auc :0.8361\n",
      "prec :0.8009\n",
      "recall :0.8598\n",
      "fpr :0.1877\n",
      "mi_f1 :0.8345\n",
      "ma_f1 :0.8344\n",
      "Total labeled 897635\n",
      "Labeled node count for 3, 9: 33643\n",
      "Labeled node count for 3, 10: 1906\n",
      "Labeled node count for 3, 11: 1460\n",
      "Epoch: 000, Loss: 0.6970, Train: 0.6989, Val: 0.7014\n",
      "Epoch: 020, Loss: 0.5103, Train: 0.7713, Val: 0.7766\n",
      "Epoch: 040, Loss: 0.4785, Train: 0.7806, Val: 0.7872\n",
      "Epoch: 060, Loss: 0.4528, Train: 0.7964, Val: 0.7983\n",
      "Epoch: 080, Loss: 0.4383, Train: 0.8033, Val: 0.8059\n",
      "Epoch: 100, Loss: 0.4344, Train: 0.8024, Val: 0.8005\n",
      "Epoch: 120, Loss: 0.4298, Train: 0.8029, Val: 0.8016\n",
      "Epoch: 140, Loss: 0.4286, Train: 0.8036, Val: 0.8029\n",
      "Epoch: 160, Loss: 0.4278, Train: 0.8032, Val: 0.8014\n",
      "Epoch: 180, Loss: 0.4263, Train: 0.8014, Val: 0.8019\n",
      "Epoch: 200, Loss: 0.4278, Train: 0.8038, Val: 0.8044\n",
      "tn, fp, fn, tp 807 196 122 781\n",
      "acc :0.8332\n",
      "f1 :0.8332\n",
      "auc :0.8347\n",
      "prec :0.7994\n",
      "recall :0.8649\n",
      "fpr :0.1954\n",
      "mi_f1 :0.8332\n",
      "ma_f1 :0.8331\n",
      "tn, fp, fn, tp 732 193 88 447\n",
      "acc :0.8075\n",
      "f1 :0.8103\n",
      "auc :0.8134\n",
      "prec :0.6984\n",
      "recall :0.8355\n",
      "fpr :0.2086\n",
      "mi_f1 :0.8075\n",
      "ma_f1 :0.7999\n",
      "Total labeled 897635\n",
      "Labeled node count for 4, 10: 32652\n",
      "Labeled node count for 4, 11: 1530\n",
      "Labeled node count for 4, 12: 2044\n",
      "Epoch: 000, Loss: 0.7079, Train: 0.5714, Val: 0.5652\n",
      "Epoch: 020, Loss: 0.5195, Train: 0.7618, Val: 0.7579\n",
      "Epoch: 040, Loss: 0.4717, Train: 0.7846, Val: 0.7833\n",
      "Epoch: 060, Loss: 0.4424, Train: 0.8010, Val: 0.7985\n",
      "Epoch: 080, Loss: 0.4327, Train: 0.8033, Val: 0.8043\n",
      "Epoch: 100, Loss: 0.4289, Train: 0.8039, Val: 0.8040\n",
      "Epoch: 120, Loss: 0.4275, Train: 0.8022, Val: 0.8038\n",
      "Epoch: 140, Loss: 0.4264, Train: 0.8038, Val: 0.8049\n",
      "Epoch: 160, Loss: 0.4263, Train: 0.8023, Val: 0.8031\n",
      "Epoch: 180, Loss: 0.4264, Train: 0.8037, Val: 0.8040\n",
      "Epoch: 200, Loss: 0.4263, Train: 0.8015, Val: 0.8031\n",
      "tn, fp, fn, tp 730 229 73 498\n",
      "acc :0.8026\n",
      "f1 :0.8057\n",
      "auc :0.8167\n",
      "prec :0.6850\n",
      "recall :0.8722\n",
      "fpr :0.2388\n",
      "mi_f1 :0.8026\n",
      "ma_f1 :0.7980\n",
      "tn, fp, fn, tp 896 260 157 731\n",
      "acc :0.7960\n",
      "f1 :0.7968\n",
      "auc :0.7991\n",
      "prec :0.7376\n",
      "recall :0.8232\n",
      "fpr :0.2249\n",
      "mi_f1 :0.7960\n",
      "ma_f1 :0.7947\n"
     ]
    }
   ],
   "source": [
    "cuda_device = 3\n",
    "torch.manual_seed(42)\n",
    "from src.utils import score\n",
    "\n",
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    mask = data.train_mask\n",
    "    loss = F.cross_entropy(out[mask], data.y[mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    pred = model(data.x, data.edge_index).argmax(dim=-1)\n",
    "\n",
    "    accs = []\n",
    "    for split in ['train_mask', 'val_mask']:\n",
    "        mask = data[split]\n",
    "        acc = (pred[mask] == data.y[mask]).sum() / mask.sum()\n",
    "        accs.append(float(acc))\n",
    "    return accs\n",
    "\n",
    "def experiment(model,start,end,test_list, model_type):\n",
    "    kg_path = lambda graph_name: f'./data/{graph_name}'\n",
    "\n",
    "    dataset = tl.DNS(root=kg_path('DNS_2m'), start=start, end=end, test_list=test_list, balance_gt=False, domain_file='domains2.csv')\n",
    "    data = to_homogeneous(dataset.train_data) # training data\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        torch.cuda.set_device(cuda_device)\n",
    "\n",
    "        data, model = data.to(device), model.to(device)\n",
    "\n",
    "    with torch.no_grad():  # Initialize lazy modules.\n",
    "        out = model(data.x, data.edge_index)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "\n",
    "    for epoch in range(0, 201):\n",
    "        loss = train(model, data, optimizer)\n",
    "        train_acc, val_acc = test(model,data)\n",
    "        if epoch % 20 == 0:\n",
    "            print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, '\n",
    "                f'Val: {val_acc:.4f}')\n",
    "        \n",
    "    model.eval()\n",
    "    for index, test_data in enumerate(dataset.test_data):\n",
    "        test_data = to_homogeneous(test_data)\n",
    "        test_data = test_data.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(test_data.x, test_data.edge_index).argmax(dim=-1)\n",
    "        mask = test_data['val_mask']\n",
    "        scores = score(pred[mask],test_data.y[mask])\n",
    "        with open(\"results_copy.csv\", \"a\") as logger:\n",
    "            logger.write(\"{},{},{},{},\".format(model_type,start,end,index))\n",
    "            logger.write(\",\".join(str(x) for x in scores.values()))\n",
    "            logger.write('\\n')\n",
    "\n",
    "\n",
    "        for metric, val in scores.items():\n",
    "            print(metric, ':{:.4f}'.format(val))\n",
    "    \n",
    "for model_type in ['gcn','sage','homogat']:\n",
    "    for i in range(5):\n",
    "        # model_type='gcn'  \n",
    "        data.x.size(1) \n",
    "        model = GNN(data.x.size(1), dim=64, num_classes=2,\n",
    "                  num_layers=2, model_type=model_type)\n",
    "        experiment(model,i,i+6,[i+7,i+8], model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
